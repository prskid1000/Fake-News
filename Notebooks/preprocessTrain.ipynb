{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 141191)\t0.05130593960615016\n",
      "  (0, 79387)\t0.04866722110598677\n",
      "  (0, 17955)\t0.02377428996247981\n",
      "  (0, 23163)\t0.026353389454324895\n",
      "  (0, 43433)\t0.019759052404340477\n",
      "  (0, 25644)\t0.06306549730890362\n",
      "  (0, 46561)\t0.01633352304463761\n",
      "  (0, 85716)\t0.04269765161864034\n",
      "  (0, 77509)\t0.028366499798749317\n",
      "  (0, 141132)\t0.012806063593094316\n",
      "  (0, 651)\t0.03706193139189739\n",
      "  (0, 123358)\t0.03092859656702817\n",
      "  (0, 61301)\t0.024284349471746115\n",
      "  (0, 116020)\t0.029329324673425985\n",
      "  (0, 111599)\t0.03232937633557134\n",
      "  (0, 120233)\t0.021519399583224914\n",
      "  (0, 32599)\t0.02690681147124843\n",
      "  (0, 72952)\t0.02329858629432371\n",
      "  (0, 131585)\t0.04236038835391851\n",
      "  (0, 22198)\t0.05280336918957663\n",
      "  (0, 21163)\t0.04002435879196746\n",
      "  (0, 86873)\t0.03337007622130158\n",
      "  (0, 130658)\t0.02523526052954641\n",
      "  (0, 122279)\t0.032816126323139565\n",
      "  (0, 24041)\t0.021867804321973338\n",
      "  :\t:\n",
      "  (20760, 43433)\t0.016284977426627274\n",
      "  (20760, 46561)\t0.013461731293388825\n",
      "  (20760, 141132)\t0.010554476615066968\n",
      "  (20760, 61301)\t0.04002925594557768\n",
      "  (20760, 66046)\t0.043762037701255994\n",
      "  (20760, 87815)\t0.01615384221342774\n",
      "  (20760, 132680)\t0.030434684488107686\n",
      "  (20760, 78846)\t0.017418823701888853\n",
      "  (20760, 96495)\t0.019344631385638706\n",
      "  (20760, 123122)\t0.012916842894535737\n",
      "  (20760, 1225)\t0.021145866701484623\n",
      "  (20760, 124676)\t0.04891585186594214\n",
      "  (20760, 76194)\t0.014477805992020396\n",
      "  (20760, 52176)\t0.014328901650050413\n",
      "  (20760, 104817)\t0.02238269301900947\n",
      "  (20760, 52266)\t0.011435409801066398\n",
      "  (20760, 85445)\t0.04442882740443369\n",
      "  (20760, 140059)\t0.011992364098092748\n",
      "  (20760, 141142)\t0.01965183942027251\n",
      "  (20760, 78103)\t0.015506116449995767\n",
      "  (20760, 73793)\t0.01348366135764236\n",
      "  (20760, 74373)\t0.011843521802763353\n",
      "  (20760, 34029)\t0.042991290950355504\n",
      "  (20760, 140117)\t0.04357992924270061\n",
      "  (20760, 89124)\t0.014605338428407234\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/train.csv\", header=0, index_col=0)\n",
    "df_1 = df_1.drop(['title','author'], axis = 1)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['text'] = df_1['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "newsTfidfVector = tfidfVectorizer.fit_transform(df_1['text'])  \n",
    "\n",
    "\n",
    "np.savez_compressed(\"../Dataset/train.npz\",news = newsTfidfVector, label = df_1['label'])\n",
    "\n",
    "data = np.load(\"../Dataset/train.npz\", allow_pickle=True)\n",
    "#print(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9481)\t0.14283909334272807\n",
      "  (0, 6802)\t0.12328078335297896\n",
      "  (0, 8067)\t0.2739066255525978\n",
      "  (0, 8838)\t0.22782987755179446\n",
      "  (0, 9493)\t0.2342171964115021\n",
      "  (0, 3365)\t0.33726858866710235\n",
      "  (0, 8627)\t0.23539722990777107\n",
      "  (0, 8719)\t0.2739066255525978\n",
      "  (0, 2618)\t0.21762962721933424\n",
      "  (0, 2129)\t0.26237880791406015\n",
      "  (0, 2353)\t0.35509474400224017\n",
      "  (0, 2681)\t0.33726858866710235\n",
      "  (0, 3874)\t0.23307347745953963\n",
      "  (0, 2354)\t0.13482461820061653\n",
      "  (0, 7815)\t0.19790727186114862\n",
      "  (0, 2265)\t0.19969884435876947\n",
      "  (0, 1414)\t0.17557774918388708\n",
      "  (1, 8723)\t0.42092558755234394\n",
      "  (1, 9674)\t0.3671877255901785\n",
      "  (1, 8001)\t0.31197936132555826\n",
      "  (1, 7810)\t0.4133582258544512\n",
      "  (1, 8854)\t0.3888249692472402\n",
      "  (1, 8627)\t0.34084169114160684\n",
      "  (1, 2354)\t0.39043663252132893\n",
      "  (2, 764)\t0.3665638987185059\n",
      "  :\t:\n",
      "  (6417, 9930)\t0.3115977441585073\n",
      "  (6418, 2813)\t0.38404093038386816\n",
      "  (6418, 1343)\t0.40056211545542475\n",
      "  (6418, 2169)\t0.3500065833992452\n",
      "  (6418, 7746)\t0.3500065833992452\n",
      "  (6418, 2)\t0.39160991121838334\n",
      "  (6418, 5152)\t0.3376667475112238\n",
      "  (6418, 6973)\t0.2524957825925164\n",
      "  (6418, 3798)\t0.34047329328429893\n",
      "  (6419, 9814)\t0.331177619287835\n",
      "  (6419, 5494)\t0.3355508195707381\n",
      "  (6419, 90)\t0.33331666965407286\n",
      "  (6419, 10402)\t0.24987001440264872\n",
      "  (6419, 8718)\t0.312214774480055\n",
      "  (6419, 3423)\t0.2584553302672947\n",
      "  (6419, 101)\t0.22461296743265838\n",
      "  (6419, 7441)\t0.25652201124605456\n",
      "  (6419, 4948)\t0.23190197686224154\n",
      "  (6419, 5649)\t0.25699841384049366\n",
      "  (6419, 6375)\t0.18684672162319219\n",
      "  (6419, 2336)\t0.2200516442947928\n",
      "  (6419, 9518)\t0.18441710695280294\n",
      "  (6419, 1368)\t0.13976140358050612\n",
      "  (6419, 6267)\t0.1489063257750822\n",
      "  (6419, 1366)\t0.2116201947139279\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/train2.csv\", header=0, index_col=0)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "label = []\n",
    "for i in df_2['label']:\n",
    "    if(i == 'real'):\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "df_2['label'] = label\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "newsTfidfVector = tfidfVectorizer.fit_transform(df_2['tweet'])  \n",
    "\n",
    "\n",
    "np.savez_compressed(\"../Dataset/train2.npz\",news = newsTfidfVector, label = df_2['label'])\n",
    "\n",
    "data = np.load(\"../Dataset/train2.npz\", allow_pickle=True)\n",
    "#print(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
