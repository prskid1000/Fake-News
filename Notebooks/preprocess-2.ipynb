{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  label\n",
      "id                                                          \n",
      "1   currently general death different small explic...      1\n",
      "2                            small rise last southern      1\n",
      "3   politically correct woman almost pandemic excu...      0\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/train2.csv\", header=0, index_col=0)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['tweet'] = df_1['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_1['label'] = df_1['label'].apply(lambda x: 1 if (x == 'real') else 0)\n",
    "\n",
    "print(df_1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  label\n",
      "id                                                          \n",
      "1                         converting affected country      0\n",
      "2   people diamond princess cruise ship tested neg...      0\n",
      "3                             bacterium virus aspirin      0\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/test2.csv\", header=0, index_col=0)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_2['label'] = df_2['label'].apply(lambda x: 1 if (x == 'real') else 0)\n",
    "\n",
    "print(df_2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(use_idf=True,stop_words='english')\n",
    "tfidfVectorizer.fit(df_1['tweet'])  \n",
    "tfidfVectorizer.fit(df_2['tweet']) \n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = tfidfVectorizer.transform(df_1['tweet']) \n",
    "testTfidfVector = tfidfVectorizer.transform(df_2['tweet']) \n",
    "\n",
    "pickle.dump(tfidfVectorizer, open(\"../Dataset/tfidf_vectorizer-2.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(trainTfidfVector, open(\"../Dataset/tfidf_train-2.pickle\", \"wb\"))\n",
    "pickle.dump(df_1['label'], open(\"../Dataset/label_train-2.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(testTfidfVector, open(\"../Dataset/tfidf_test-2.pickle\", \"wb\"))\n",
    "pickle.dump(df_2['label'], open(\"../Dataset/label_test-2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = pickle.load(open(\"../Dataset/tfidf_vectorizer-2.pickle\", \"rb\"))\n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = pickle.load(open(\"../Dataset/tfidf_train-2.pickle\", \"rb\"))\n",
    "df_train = pd.DataFrame(data = trainTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_train)\n",
    "\n",
    "testTfidfVector = pickle.load(open(\"../Dataset/tfidf_test-2.pickle\", \"rb\"))\n",
    "df_test = pd.DataFrame(data = testTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_test)\n",
    "\n",
    "trainLabels = pickle.load(open(\"../Dataset/label_train-2.pickle\", \"rb\"))\n",
    "#print(trainLabels)\n",
    "\n",
    "testLabels = pickle.load(open(\"../Dataset/label_test-2.pickle\", \"rb\"))\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
