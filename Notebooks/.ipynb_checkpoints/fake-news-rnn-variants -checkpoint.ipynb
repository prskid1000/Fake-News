{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T07:55:02.092657Z",
     "iopub.status.busy": "2021-11-30T07:55:02.092083Z",
     "iopub.status.idle": "2021-11-30T07:55:02.192473Z",
     "shell.execute_reply": "2021-11-30T07:55:02.191554Z",
     "shell.execute_reply.started": "2021-11-30T07:55:02.092584Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:08:54.336056Z",
     "iopub.status.busy": "2021-11-30T08:08:54.335785Z",
     "iopub.status.idle": "2021-11-30T08:09:01.003898Z",
     "shell.execute_reply": "2021-11-30T08:09:01.00302Z",
     "shell.execute_reply.started": "2021-11-30T08:08:54.336029Z"
    }
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "!pip install contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import gensim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow  as tf \n",
    "from tensorflow.keras.models import Model, load_model\n",
    "#from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "#from tokenizers import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T07:55:29.665342Z",
     "iopub.status.busy": "2021-11-30T07:55:29.664946Z",
     "iopub.status.idle": "2021-11-30T07:55:32.793721Z",
     "shell.execute_reply": "2021-11-30T07:55:32.792852Z",
     "shell.execute_reply.started": "2021-11-30T07:55:29.665307Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"/kaggle/input/fake-news/train.csv\", header=0, index_col=0)\n",
    "df_t = pd.read_csv(\"/kaggle/input/fake-news/test.csv\", header=0, index_col=0)\n",
    "df_2 = df_2.drop(['title','author'], axis = 1)\n",
    "df_t = df_t.drop(['title','author'], axis = 1)\n",
    "df_2.dropna(inplace = True)\n",
    "df_t.fillna('',inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T07:55:32.796083Z",
     "iopub.status.busy": "2021-11-30T07:55:32.795847Z",
     "iopub.status.idle": "2021-11-30T07:57:30.707834Z",
     "shell.execute_reply": "2021-11-30T07:57:30.707113Z",
     "shell.execute_reply.started": "2021-11-30T07:55:32.79606Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text_col ):\n",
    "    text_col = text_col.apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "    text_col = text_col.apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_col = text_col.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    text_col = text_col.apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "    return text_col\n",
    "df_2[\"text\"] = clean_text(df_2[\"text\"])\n",
    "df_t[\"text\"] = clean_text(df_t[\"text\"])\n",
    "df_2['label'] = df_2['label'].apply(lambda x: int(x))\n",
    "y = df_2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:10:56.274658Z",
     "iopub.status.busy": "2021-11-30T08:10:56.274039Z",
     "iopub.status.idle": "2021-11-30T08:10:56.278739Z",
     "shell.execute_reply": "2021-11-30T08:10:56.277702Z",
     "shell.execute_reply.started": "2021-11-30T08:10:56.274618Z"
    }
   },
   "outputs": [],
   "source": [
    "#y = df_2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T07:59:17.068188Z",
     "iopub.status.busy": "2021-11-30T07:59:17.067439Z",
     "iopub.status.idle": "2021-11-30T07:59:56.920621Z",
     "shell.execute_reply": "2021-11-30T07:59:56.919877Z",
     "shell.execute_reply.started": "2021-11-30T07:59:17.068122Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "x = []\n",
    "x_test = []\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "for words in df_2['text']:\n",
    "        tmp = []\n",
    "        fil_wor = [wordnet_lemmatizer.lemmatize(word, 'n') for word in words if word in english_words]\n",
    "        tmp.extend(fil_wor)\n",
    "        x.append(tmp)\n",
    "        \n",
    "for words in df_t['text']:\n",
    "        tmp = []\n",
    "        fil_wor = [wordnet_lemmatizer.lemmatize(word, 'n') for word in words if word in english_words]\n",
    "        tmp.extend(fil_wor)\n",
    "        x_test.append(tmp)\n",
    "        \n",
    "df_2[\"text\"] = x\n",
    "df_t[\"text\"] = x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:00:04.618098Z",
     "iopub.status.busy": "2021-11-30T08:00:04.617745Z",
     "iopub.status.idle": "2021-11-30T08:01:03.866077Z",
     "shell.execute_reply": "2021-11-30T08:01:03.864495Z",
     "shell.execute_reply.started": "2021-11-30T08:00:04.618057Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating word embedding\n",
    "x_all = x.copy()\n",
    "for y in  range(len(x_test)):\n",
    "    x_all.append(x_test[y])\n",
    "#n of vectors we are generating\n",
    "EMBEDDING_DIM = 100\n",
    "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
    "w2v_model = gensim.models.Word2Vec(sentences=x_all, vector_size=EMBEDDING_DIM, window=5, min_count=1)\n",
    "print(len(w2v_model.wv))\n",
    "\n",
    "#testing a word embedding\n",
    "w2v_model.wv[\"liberty\"]\n",
    "\n",
    "#similarity between words\n",
    "word = 'people'\n",
    "w2v_model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:09:22.400063Z",
     "iopub.status.busy": "2021-11-30T08:09:22.399778Z",
     "iopub.status.idle": "2021-11-30T08:09:30.010871Z",
     "shell.execute_reply": "2021-11-30T08:09:30.010168Z",
     "shell.execute_reply.started": "2021-11-30T08:09:22.400026Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "print(x[0][:10])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "for word, num in word_index.items():\n",
    "    print(f\"{word} -> {num}\")\n",
    "    if num == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:09:52.874798Z",
     "iopub.status.busy": "2021-11-30T08:09:52.874333Z",
     "iopub.status.idle": "2021-11-30T08:09:54.231952Z",
     "shell.execute_reply": "2021-11-30T08:09:54.231182Z",
     "shell.execute_reply.started": "2021-11-30T08:09:52.874763Z"
    }
   },
   "outputs": [],
   "source": [
    "#padding\n",
    "maxlen = 700\n",
    "x = pad_sequences(x, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:09:58.862688Z",
     "iopub.status.busy": "2021-11-30T08:09:58.861963Z",
     "iopub.status.idle": "2021-11-30T08:09:58.988647Z",
     "shell.execute_reply": "2021-11-30T08:09:58.987874Z",
     "shell.execute_reply.started": "2021-11-30T08:09:58.86265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix, vocab_size\n",
    "\n",
    "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
    "embedding_vectors, vocab_size = get_weight_matrix(w2v_model.wv, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:11:48.915079Z",
     "iopub.status.busy": "2021-11-30T08:11:48.914815Z",
     "iopub.status.idle": "2021-11-30T08:11:49.428709Z",
     "shell.execute_reply": "2021-11-30T08:11:49.427849Z",
     "shell.execute_reply.started": "2021-11-30T08:11:48.915052Z"
    }
   },
   "outputs": [],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(units=128,dropout=0.2, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=128,dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:11:57.05489Z",
     "iopub.status.busy": "2021-11-30T08:11:57.054631Z",
     "iopub.status.idle": "2021-11-30T08:29:33.198818Z",
     "shell.execute_reply": "2021-11-30T08:29:33.198065Z",
     "shell.execute_reply.started": "2021-11-30T08:11:57.054861Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y)\n",
    "model.fit(X_train, y_train, validation_data= (X_test,y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:29:40.453967Z",
     "iopub.status.busy": "2021-11-30T08:29:40.453319Z",
     "iopub.status.idle": "2021-11-30T08:29:43.572804Z",
     "shell.execute_reply": "2021-11-30T08:29:43.571978Z",
     "shell.execute_reply.started": "2021-11-30T08:29:40.453926Z"
    }
   },
   "outputs": [],
   "source": [
    "#validation_data_performance evaluation\n",
    "y_pred = (model.predict(X_test) >= 0.5).astype(\"int\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred)) \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:29:52.904768Z",
     "iopub.status.busy": "2021-11-30T08:29:52.904003Z",
     "iopub.status.idle": "2021-11-30T08:29:55.121165Z",
     "shell.execute_reply": "2021-11-30T08:29:55.120371Z",
     "shell.execute_reply.started": "2021-11-30T08:29:52.904728Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_data_for_scoring_on_kaggle\n",
    "y_t = (model.predict(x_test) >= 0.5).astype(\"int\")\n",
    "result = pd.DataFrame({\"id\" :df_t.index, \"label\":y_t.squeeze() }, index = None )\n",
    "result.to_csv(\"result_rnn.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:29:57.68453Z",
     "iopub.status.busy": "2021-11-30T08:29:57.684121Z",
     "iopub.status.idle": "2021-11-30T08:29:57.695072Z",
     "shell.execute_reply": "2021-11-30T08:29:57.694405Z",
     "shell.execute_reply.started": "2021-11-30T08:29:57.684491Z"
    }
   },
   "outputs": [],
   "source": [
    "#let's include an attention layer in our model\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features)+ self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score),axis = 1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector  = tf.reduce_sum(context_vector, axis = 1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-29T19:30:42.040455Z",
     "iopub.status.busy": "2021-11-29T19:30:42.039663Z",
     "iopub.status.idle": "2021-11-29T19:30:42.047891Z",
     "shell.execute_reply": "2021-11-29T19:30:42.047064Z",
     "shell.execute_reply.started": "2021-11-29T19:30:42.040411Z"
    }
   },
   "outputs": [],
   "source": [
    "'''#dd attention layer to the deep learning network\n",
    "class Attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', #shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', #shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(Attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:30:14.487018Z",
     "iopub.status.busy": "2021-11-30T08:30:14.486761Z",
     "iopub.status.idle": "2021-11-30T08:40:39.487116Z",
     "shell.execute_reply": "2021-11-30T08:40:39.486402Z",
     "shell.execute_reply.started": "2021-11-30T08:30:14.486991Z"
    }
   },
   "outputs": [],
   "source": [
    "#RNN with Attention model\n",
    "sequence_input = Input(shape = (maxlen,), dtype = \"int32\")\n",
    "embedding = Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False)(sequence_input)\n",
    "dropout = Dropout(0.2)(embedding)\n",
    "\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(dropout)\n",
    "maxp = MaxPooling1D(pool_size=2)(conv1)\n",
    "#(lstm, state_h, state_c) = LSTM(units=128,return_sequences=True,dropout=0.2, return_state= True)(maxp)\n",
    "#bn1 = BatchNormalization()((lstm, state_h, state_c))\n",
    "(lstm, state_h, state_c) = LSTM(units=128,dropout=0.2, return_sequences=True, return_state= True)(maxp)\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "densee = Dense(20, activation='relu')(context_vector)\n",
    "#bn = BatchNormalization()(densee)\n",
    "dropout2 = Dropout(0.2)(densee)\n",
    "densef = Dense(1, activation='sigmoid')(dropout2)\n",
    "model = tensorflow.keras.Model(inputs = sequence_input, outputs = densef)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "display(model.summary())\n",
    "model.fit(X_train, y_train, validation_data= (X_test,y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:46:26.194451Z",
     "iopub.status.busy": "2021-11-30T08:46:26.193961Z",
     "iopub.status.idle": "2021-11-30T08:46:28.265741Z",
     "shell.execute_reply": "2021-11-30T08:46:28.26487Z",
     "shell.execute_reply.started": "2021-11-30T08:46:26.194414Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking accuracy on validation data\n",
    "y_pred = (model.predict(X_test) >= 0.5).astype(\"int\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T08:46:31.373915Z",
     "iopub.status.busy": "2021-11-30T08:46:31.373661Z",
     "iopub.status.idle": "2021-11-30T08:46:32.939174Z",
     "shell.execute_reply": "2021-11-30T08:46:32.938326Z",
     "shell.execute_reply.started": "2021-11-30T08:46:31.373888Z"
    }
   },
   "outputs": [],
   "source": [
    "y_t = (model.predict(x_test) >= 0.5).astype(\"int\")\n",
    "result = pd.DataFrame({\"id\" :df_t.index, \"label\":y_t.squeeze() }, index = None )\n",
    "result.to_csv(\"result_rnnattenion.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
