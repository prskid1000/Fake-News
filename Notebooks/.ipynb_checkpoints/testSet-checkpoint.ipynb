{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>house dem did not even see comey’s letter jaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "id                                                          \n",
       "0   house dem did not even see comey’s letter jaso...      1\n",
       "1   ever get feeling life circles roundabout rathe...      0\n",
       "2   truth might get fired october tension intellig...      1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/train.csv\", header=0, index_col=0)\n",
    "df_1 = df_1.drop(['title','author'], axis = 1)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['text'] = df_1['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cdc currently reports deaths. general discrepa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>states reported deaths small rise last tuesday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>politically correct woman uses pandemic excuse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label\n",
       "id                                                          \n",
       "1   cdc currently reports deaths. general discrepa...      1\n",
       "2   states reported deaths small rise last tuesday...      1\n",
       "3   politically correct woman uses pandemic excuse...      0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/train2.csv\", header=0, index_col=0)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "label = []\n",
    "for i in df_2['label']:\n",
    "    if(i == 'real'):\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "df_2['label'] = label\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27181\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['news'] = df_1['text'].append(df_2['tweet'])\n",
    "df['label'] = df_1['label'].append(df_2['label'])\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "newsTfidfVector = tfidfVectorizer.fit_transform(df['news'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27181, 168635)\n"
     ]
    }
   ],
   "source": [
    "print(newsTfidfVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"../Dataset/train.npz\",news = newsTfidfVector, label = df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 142247)\t0.05032711229853054\n",
      "  (0, 80025)\t0.047819402045884654\n",
      "  (0, 18119)\t0.024087828688302317\n",
      "  (0, 23371)\t0.026537716759481994\n",
      "  (0, 43780)\t0.020104896945131002\n",
      "  (0, 25867)\t0.0630004829957127\n",
      "  (0, 46928)\t0.017021675788073714\n",
      "  (0, 86416)\t0.04193058453934827\n",
      "  (0, 78134)\t0.028516005263695497\n",
      "  (0, 142187)\t0.013697948676480916\n",
      "  (0, 658)\t0.03679029681956147\n",
      "  (0, 124296)\t0.0309134285856916\n",
      "  (0, 61806)\t0.024433648607809312\n",
      "  (0, 116930)\t0.02944160193362026\n",
      "  (0, 112479)\t0.03227252793624632\n",
      "  (0, 121153)\t0.021984920581528466\n",
      "  (0, 32881)\t0.02713936272398327\n",
      "  (0, 73542)\t0.02370565471054426\n",
      "  (0, 132583)\t0.04182569367831718\n",
      "  (0, 22404)\t0.05327264138384922\n",
      "  (0, 21356)\t0.03960564433139905\n",
      "  (0, 87586)\t0.0332817364854396\n",
      "  (0, 131651)\t0.025519146971172336\n",
      "  (0, 123208)\t0.03273344583590353\n",
      "  (0, 24260)\t0.022310374856931364\n",
      "  :\t:\n",
      "  (27178, 23039)\t0.3534861758612966\n",
      "  (27179, 34)\t0.5217297198648224\n",
      "  (27179, 27970)\t0.44975744283609836\n",
      "  (27179, 68726)\t0.3291752564109035\n",
      "  (27179, 35920)\t0.30633557469608363\n",
      "  (27179, 106405)\t0.2819553702797375\n",
      "  (27179, 19563)\t0.33062165301020024\n",
      "  (27179, 97713)\t0.27563419573374254\n",
      "  (27179, 48687)\t0.24193964152072267\n",
      "  (27180, 102750)\t0.35441873265172613\n",
      "  (27180, 64718)\t0.2964659568478566\n",
      "  (27180, 43830)\t0.27783229470470117\n",
      "  (27180, 74702)\t0.3520857021193308\n",
      "  (27180, 143144)\t0.31431768451258685\n",
      "  (27180, 77201)\t0.24476616916671223\n",
      "  (27180, 129495)\t0.1980250138205583\n",
      "  (27180, 19959)\t0.18136860541376076\n",
      "  (27180, 89035)\t0.16198710246722153\n",
      "  (27180, 1161)\t0.3082446728216707\n",
      "  (27180, 1260)\t0.23820931665539877\n",
      "  (27180, 119668)\t0.21050526745333292\n",
      "  (27180, 87145)\t0.1024852071117363\n",
      "  (27180, 133717)\t0.27035052150548267\n",
      "  (27180, 30609)\t0.15289752489323283\n",
      "  (27180, 19949)\t0.1588854623136621\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"../Dataset/train.npz\", allow_pickle=True)\n",
    "print(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
