{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9211)\t0.025658237077260462\n",
      "  (0, 34907)\t0.01870903328764651\n",
      "  (0, 17818)\t0.019461215933817708\n",
      "  (0, 9263)\t0.03276799594680187\n",
      "  (0, 28637)\t0.046393042779189056\n",
      "  (0, 57599)\t0.0442704381837321\n",
      "  (0, 32848)\t0.029978771122589478\n",
      "  (0, 30862)\t0.026582859001568586\n",
      "  (0, 54061)\t0.01885650746412142\n",
      "  (0, 54063)\t0.01912145016402184\n",
      "  (0, 16363)\t0.028010147835660963\n",
      "  (0, 4353)\t0.01769107404019313\n",
      "  (0, 59267)\t0.021244054759478798\n",
      "  (0, 52645)\t0.037013205137715784\n",
      "  (0, 21071)\t0.03187857668715643\n",
      "  (0, 48483)\t0.022553556165021565\n",
      "  (0, 72392)\t0.01891118145521652\n",
      "  (0, 11954)\t0.033170886001506944\n",
      "  (0, 37439)\t0.01398682171346248\n",
      "  (0, 32423)\t0.01816938358972519\n",
      "  (0, 41691)\t0.01797048865088527\n",
      "  (0, 7474)\t0.029043321920247923\n",
      "  (0, 21218)\t0.013554612801931336\n",
      "  (0, 62760)\t0.028856338219909745\n",
      "  (0, 15215)\t0.09278608555837811\n",
      "  :\t:\n",
      "  (5192, 23478)\t0.013401636539745685\n",
      "  (5192, 60486)\t0.0213954433326866\n",
      "  (5192, 9601)\t0.013645001269564406\n",
      "  (5192, 3279)\t0.019962975321489003\n",
      "  (5192, 50514)\t0.013820824278827342\n",
      "  (5192, 60494)\t0.051832878331176106\n",
      "  (5192, 54341)\t0.027198877493212377\n",
      "  (5192, 72432)\t0.01625064908589955\n",
      "  (5192, 3962)\t0.018455152263925606\n",
      "  (5192, 43162)\t0.01472637264646993\n",
      "  (5192, 6078)\t0.015197926482433245\n",
      "  (5192, 28074)\t0.014265344917677655\n",
      "  (5192, 57787)\t0.05405228095377611\n",
      "  (5192, 38955)\t0.017027980339261516\n",
      "  (5192, 72070)\t0.03638917772661196\n",
      "  (5192, 51932)\t0.0179307807042426\n",
      "  (5192, 59274)\t0.014363670204715953\n",
      "  (5192, 70347)\t0.012801976895709792\n",
      "  (5192, 50619)\t0.01660678113292865\n",
      "  (5192, 2649)\t0.04569085327717929\n",
      "  (5192, 46774)\t0.031678439121260185\n",
      "  (5192, 43563)\t0.03497391209936226\n",
      "  (5192, 58413)\t0.029193055073471363\n",
      "  (5192, 44952)\t0.008652647547099782\n",
      "  (5192, 56945)\t0.032619500515068484\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/test.csv\", header=0, index_col=0)\n",
    "df_1 = df_1.drop(['title','author'], axis = 1)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['text'] = df_1['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "newsTfidfVector = tfidfVectorizer.fit_transform(df_1['text'])  \n",
    "\n",
    "\n",
    "np.savez_compressed(\"../Dataset/test.npz\",news = newsTfidfVector, label = df_1['label'])\n",
    "\n",
    "data = np.load(\"../Dataset/test.npz\", allow_pickle=True)\n",
    "#print(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1177)\t0.2768017945236641\n",
      "  (0, 111)\t0.3459558250374142\n",
      "  (0, 3400)\t0.37353187181897096\n",
      "  (0, 4236)\t0.46796079194564794\n",
      "  (0, 2720)\t0.41420019412171555\n",
      "  (0, 1128)\t0.42729246964290335\n",
      "  (0, 841)\t0.301730535717187\n",
      "  (1, 4973)\t0.1628789313348569\n",
      "  (1, 5523)\t0.27109763727056235\n",
      "  (1, 3931)\t0.19167828535888534\n",
      "  (1, 1051)\t0.1718494813006492\n",
      "  (1, 2912)\t0.3442928091015757\n",
      "  (1, 2748)\t0.2996859527845773\n",
      "  (1, 5259)\t0.1640159584899815\n",
      "  (1, 3465)\t0.24608223541207405\n",
      "  (1, 5257)\t0.20655950884454544\n",
      "  (1, 2694)\t0.3627328421281201\n",
      "  (1, 1232)\t0.3210611149688782\n",
      "  (1, 4032)\t0.3442928091015757\n",
      "  (1, 1451)\t0.3442928091015757\n",
      "  (1, 3783)\t0.15329560912255064\n",
      "  (2, 310)\t0.49511488123434727\n",
      "  (2, 5409)\t0.4621518084957788\n",
      "  (2, 5670)\t0.29086108227542873\n",
      "  (2, 408)\t0.5309409496839576\n",
      "  :\t:\n",
      "  (2137, 4579)\t0.523874282728617\n",
      "  (2137, 3022)\t0.31103004640898946\n",
      "  (2137, 2981)\t0.1731896402435438\n",
      "  (2137, 3783)\t0.11662693084424793\n",
      "  (2138, 527)\t0.616955285907173\n",
      "  (2138, 1117)\t0.51106949245055\n",
      "  (2138, 3660)\t0.41910458562392805\n",
      "  (2138, 4593)\t0.3272102071957206\n",
      "  (2138, 3783)\t0.2746979717680215\n",
      "  (2139, 2086)\t0.3336498272053958\n",
      "  (2139, 3670)\t0.32617122646447944\n",
      "  (2139, 3443)\t0.3042432253353512\n",
      "  (2139, 2382)\t0.25243707218923567\n",
      "  (2139, 4959)\t0.32617122646447944\n",
      "  (2139, 1269)\t0.21446160181809318\n",
      "  (2139, 3548)\t0.17748647355847652\n",
      "  (2139, 4228)\t0.2392442423660065\n",
      "  (2139, 5350)\t0.17375271837875678\n",
      "  (2139, 5483)\t0.24539358533852895\n",
      "  (2139, 1145)\t0.1369375733591169\n",
      "  (2139, 731)\t0.2676305189657264\n",
      "  (2139, 3483)\t0.28547903387137874\n",
      "  (2139, 4364)\t0.18118143235583337\n",
      "  (2139, 2913)\t0.2369873296894979\n",
      "  (2139, 1051)\t0.1833221416868241\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/test2.csv\", header=0, index_col=0)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "label = []\n",
    "for i in df_2['label']:\n",
    "    if(i == 'real'):\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "df_2['label'] = label\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['tweet'] = df_2['tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "newsTfidfVector = tfidfVectorizer.fit_transform(df_2['tweet'])  \n",
    "\n",
    "\n",
    "np.savez_compressed(\"../Dataset/test2.npz\",news = newsTfidfVector, label = df_2['label'])\n",
    "\n",
    "data = np.load(\"../Dataset/test2.npz\", allow_pickle=True)\n",
    "#print(data['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
