{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-11-30T07:55:02.092083Z","iopub.execute_input":"2021-11-30T07:55:02.092657Z","iopub.status.idle":"2021-11-30T07:55:02.192473Z","shell.execute_reply.started":"2021-11-30T07:55:02.092584Z","shell.execute_reply":"2021-11-30T07:55:02.191554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dependencies\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!pip install contractions\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('words')\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport contractions\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport gensim\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow  as tf \nfrom tensorflow.keras.models import Model, load_model\n#from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\n#from tokenizers import Tokenizer\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:08:54.335785Z","iopub.execute_input":"2021-11-30T08:08:54.336056Z","iopub.status.idle":"2021-11-30T08:09:01.003898Z","shell.execute_reply.started":"2021-11-30T08:08:54.336029Z","shell.execute_reply":"2021-11-30T08:09:01.00302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2 = pd.read_csv(\"/kaggle/input/fake-news/train.csv\", header=0, index_col=0)\ndf_t = pd.read_csv(\"/kaggle/input/fake-news/test.csv\", header=0, index_col=0)\ndf_2 = df_2.drop(['title','author'], axis = 1)\ndf_t = df_t.drop(['title','author'], axis = 1)\ndf_2.dropna(inplace = True)\ndf_t.fillna('',inplace = True)\n#print(df_2.isnull().sum(axis = 0))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T07:55:29.664946Z","iopub.execute_input":"2021-11-30T07:55:29.665342Z","iopub.status.idle":"2021-11-30T07:55:32.793721Z","shell.execute_reply.started":"2021-11-30T07:55:29.665307Z","shell.execute_reply":"2021-11-30T07:55:32.792852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text_col ):\n    text_col = text_col.apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n    text_col = text_col.apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n    stop_words = set(stopwords.words('english'))\n    text_col = text_col.apply(lambda x: [word for word in x if word not in stop_words])\n    text_col = text_col.apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n    return text_col\ndf_2[\"text\"] = clean_text(df_2[\"text\"])\ndf_t[\"text\"] = clean_text(df_t[\"text\"])\ndf_2['label'] = df_2['label'].apply(lambda x: int(x))\ny = df_2['label']","metadata":{"execution":{"iopub.status.busy":"2021-11-30T07:55:32.795847Z","iopub.execute_input":"2021-11-30T07:55:32.796083Z","iopub.status.idle":"2021-11-30T07:57:30.707834Z","shell.execute_reply.started":"2021-11-30T07:55:32.79606Z","shell.execute_reply":"2021-11-30T07:57:30.707113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y = df_2[\"label\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:10:56.274039Z","iopub.execute_input":"2021-11-30T08:10:56.274658Z","iopub.status.idle":"2021-11-30T08:10:56.278739Z","shell.execute_reply.started":"2021-11-30T08:10:56.274618Z","shell.execute_reply":"2021-11-30T08:10:56.277702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lemmatizing\nwordnet_lemmatizer = WordNetLemmatizer()\nx = []\nx_test = []\nenglish_words = set(nltk.corpus.words.words())\nfor words in df_2['text']:\n        tmp = []\n        fil_wor = [wordnet_lemmatizer.lemmatize(word, 'n') for word in words if word in english_words]\n        tmp.extend(fil_wor)\n        x.append(tmp)\n        \nfor words in df_t['text']:\n        tmp = []\n        fil_wor = [wordnet_lemmatizer.lemmatize(word, 'n') for word in words if word in english_words]\n        tmp.extend(fil_wor)\n        x_test.append(tmp)\n        \ndf_2[\"text\"] = x\ndf_t[\"text\"] = x_test\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T07:59:17.067439Z","iopub.execute_input":"2021-11-30T07:59:17.068188Z","iopub.status.idle":"2021-11-30T07:59:56.920621Z","shell.execute_reply.started":"2021-11-30T07:59:17.068122Z","shell.execute_reply":"2021-11-30T07:59:56.919877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating word embedding\nx_all = x.copy()\nfor y in  range(len(x_test)):\n    x_all.append(x_test[y])\n#n of vectors we are generating\nEMBEDDING_DIM = 100\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences=x_all, vector_size=EMBEDDING_DIM, window=5, min_count=1)\nprint(len(w2v_model.wv))\n\n#testing a word embedding\nw2v_model.wv[\"liberty\"]\n\n#similarity between words\nword = 'people'\nw2v_model.wv.most_similar(word)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:00:04.617745Z","iopub.execute_input":"2021-11-30T08:00:04.618098Z","iopub.status.idle":"2021-11-30T08:01:03.866077Z","shell.execute_reply.started":"2021-11-30T08:00:04.618057Z","shell.execute_reply":"2021-11-30T08:01:03.864495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizing\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\n\nx = tokenizer.texts_to_sequences(x)\nx_test = tokenizer.texts_to_sequences(x_test)\nprint(x[0][:10])\n\nword_index = tokenizer.word_index\nfor word, num in word_index.items():\n    print(f\"{word} -> {num}\")\n    if num == 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:09:22.399778Z","iopub.execute_input":"2021-11-30T08:09:22.400063Z","iopub.status.idle":"2021-11-30T08:09:30.010871Z","shell.execute_reply.started":"2021-11-30T08:09:22.400026Z","shell.execute_reply":"2021-11-30T08:09:30.010168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padding\nmaxlen = 700\nx = pad_sequences(x, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:09:52.874333Z","iopub.execute_input":"2021-11-30T08:09:52.874798Z","iopub.status.idle":"2021-11-30T08:09:54.231952Z","shell.execute_reply.started":"2021-11-30T08:09:52.874763Z","shell.execute_reply":"2021-11-30T08:09:54.231182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create weight matrix from word2vec gensim model\ndef get_weight_matrix(model, vocab):\n    # total vocabulary size plus 0 for unknown words\n    vocab_size = len(vocab) + 1\n    # define weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        weight_matrix[i] = model[word]\n    return weight_matrix, vocab_size\n\n#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\nembedding_vectors, vocab_size = get_weight_matrix(w2v_model.wv, word_index)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:09:58.861963Z","iopub.execute_input":"2021-11-30T08:09:58.862688Z","iopub.status.idle":"2021-11-30T08:09:58.988647Z","shell.execute_reply.started":"2021-11-30T08:09:58.86265Z","shell.execute_reply":"2021-11-30T08:09:58.987874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(Dropout(0.2))\n#model.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n#model.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(units=128,dropout=0.2, return_sequences=True))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(units=128,dropout=0.2))\nmodel.add(BatchNormalization())\n#model.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:11:48.914815Z","iopub.execute_input":"2021-11-30T08:11:48.915079Z","iopub.status.idle":"2021-11-30T08:11:49.428709Z","shell.execute_reply.started":"2021-11-30T08:11:48.915052Z","shell.execute_reply":"2021-11-30T08:11:49.427849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y)\nmodel.fit(X_train, y_train, validation_data= (X_test,y_test), epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:11:57.054631Z","iopub.execute_input":"2021-11-30T08:11:57.05489Z","iopub.status.idle":"2021-11-30T08:29:33.198818Z","shell.execute_reply.started":"2021-11-30T08:11:57.054861Z","shell.execute_reply":"2021-11-30T08:29:33.198065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation_data_performance evaluation\ny_pred = (model.predict(X_test) >= 0.5).astype(\"int\")\nprint(accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred)) \ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,7))\nsns.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:29:40.453319Z","iopub.execute_input":"2021-11-30T08:29:40.453967Z","iopub.status.idle":"2021-11-30T08:29:43.572804Z","shell.execute_reply.started":"2021-11-30T08:29:40.453926Z","shell.execute_reply":"2021-11-30T08:29:43.571978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_data_for_scoring_on_kaggle\ny_t = (model.predict(x_test) >= 0.5).astype(\"int\")\nresult = pd.DataFrame({\"id\" :df_t.index, \"label\":y_t.squeeze() }, index = None )\nresult.to_csv(\"result_rnn.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:29:52.904003Z","iopub.execute_input":"2021-11-30T08:29:52.904768Z","iopub.status.idle":"2021-11-30T08:29:55.121165Z","shell.execute_reply.started":"2021-11-30T08:29:52.904728Z","shell.execute_reply":"2021-11-30T08:29:55.120371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's include an attention layer in our model\nclass Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n    def call(self, features, hidden):\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        score = tf.nn.tanh(self.W1(features)+ self.W2(hidden_with_time_axis))\n        attention_weights = tf.nn.softmax(self.V(score),axis = 1)\n        context_vector = attention_weights * features\n        context_vector  = tf.reduce_sum(context_vector, axis = 1)\n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:29:57.684121Z","iopub.execute_input":"2021-11-30T08:29:57.68453Z","iopub.status.idle":"2021-11-30T08:29:57.695072Z","shell.execute_reply.started":"2021-11-30T08:29:57.684491Z","shell.execute_reply":"2021-11-30T08:29:57.694405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#dd attention layer to the deep learning network\nclass Attention(Layer):\n    def __init__(self,**kwargs):\n        super(Attention,self).__init__(**kwargs)\n \n    def build(self,input_shape):\n        self.W=self.add_weight(name='attention_weight', #shape=(input_shape[-1],1), \n                               initializer='random_normal', trainable=True)\n        self.b=self.add_weight(name='attention_bias', #shape=(input_shape[1],1), \n                               initializer='zeros', trainable=True)        \n        super(Attention, self).build(input_shape)\n \n    def call(self,x):\n        # Alignment scores. Pass them through tanh function\n        e = K.tanh(K.dot(x,self.W)+self.b)\n        # Remove dimension of size 1\n        e = K.squeeze(e, axis=-1)   \n        # Compute the weights\n        alpha = K.softmax(e)\n        # Reshape to tensorFlow format\n        alpha = K.expand_dims(alpha, axis=-1)\n        # Compute the context vector\n        context = x * alpha\n        context = K.sum(context, axis=1)\n        return context'''","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:30:42.039663Z","iopub.execute_input":"2021-11-29T19:30:42.040455Z","iopub.status.idle":"2021-11-29T19:30:42.047891Z","shell.execute_reply.started":"2021-11-29T19:30:42.040411Z","shell.execute_reply":"2021-11-29T19:30:42.047064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RNN with Attention model\nsequence_input = Input(shape = (maxlen,), dtype = \"int32\")\nembedding = Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False)(sequence_input)\ndropout = Dropout(0.2)(embedding)\n\nconv1 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(dropout)\nmaxp = MaxPooling1D(pool_size=2)(conv1)\n#(lstm, state_h, state_c) = LSTM(units=128,return_sequences=True,dropout=0.2, return_state= True)(maxp)\n#bn1 = BatchNormalization()((lstm, state_h, state_c))\n(lstm, state_h, state_c) = LSTM(units=128,dropout=0.2, return_sequences=True, return_state= True)(maxp)\ncontext_vector, attention_weights = Attention(10)(lstm, state_h)\ndensee = Dense(20, activation='relu')(context_vector)\n#bn = BatchNormalization()(densee)\ndropout2 = Dropout(0.2)(densee)\ndensef = Dense(1, activation='sigmoid')(dropout2)\nmodel = tensorflow.keras.Model(inputs = sequence_input, outputs = densef)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\ndisplay(model.summary())\nmodel.fit(X_train, y_train, validation_data= (X_test,y_test), epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:30:14.486761Z","iopub.execute_input":"2021-11-30T08:30:14.487018Z","iopub.status.idle":"2021-11-30T08:40:39.487116Z","shell.execute_reply.started":"2021-11-30T08:30:14.486991Z","shell.execute_reply":"2021-11-30T08:40:39.486402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking accuracy on validation data\ny_pred = (model.predict(X_test) >= 0.5).astype(\"int\")\nprint(accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:46:26.193961Z","iopub.execute_input":"2021-11-30T08:46:26.194451Z","iopub.status.idle":"2021-11-30T08:46:28.265741Z","shell.execute_reply.started":"2021-11-30T08:46:26.194414Z","shell.execute_reply":"2021-11-30T08:46:28.26487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_t = (model.predict(x_test) >= 0.5).astype(\"int\")\nresult = pd.DataFrame({\"id\" :df_t.index, \"label\":y_t.squeeze() }, index = None )\nresult.to_csv(\"result_rnnattenion.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T08:46:31.373661Z","iopub.execute_input":"2021-11-30T08:46:31.373915Z","iopub.status.idle":"2021-11-30T08:46:32.939174Z","shell.execute_reply.started":"2021-11-30T08:46:31.373888Z","shell.execute_reply":"2021-11-30T08:46:32.938326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}