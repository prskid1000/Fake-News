{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/7b/19437c9a5bd16e1bb3a5bf43f7655e341882befceae0122e43c8e2c21e1e/anyascii-0.3.0-py3-none-any.whl\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/train.csv\", header=0, index_col=0)\n",
    "df_1 = df_1.drop(['title','author'], axis = 1)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['text'] = df_1['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#print(df_1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/test.csv\", header=0, index_col=0)\n",
    "df_2 = df_2.drop(['title','author'], axis = 1)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['text'] = df_2['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_2['label'] = df_2['label'].apply(lambda x: int(x))\n",
    "\n",
    "#print(df_2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(use_idf=True,stop_words='english')\n",
    "tfidfVectorizer.fit(df_1['text'])  \n",
    "tfidfVectorizer.fit(df_2['text']) \n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = tfidfVectorizer.transform(df_1['text']) \n",
    "testTfidfVector = tfidfVectorizer.transform(df_2['text']) \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_1['text'])\n",
    "tokenizer.fit_on_texts(df_2['text'])\n",
    "\n",
    "trainSequence = np.array([np.array(seq) for seq in pad_sequences(tokenizer.texts_to_sequences(df_1['text']))])\n",
    "testSequence = np.array([np.array(seq) for seq in pad_sequences(tokenizer.texts_to_sequences(df_2['text']))])\n",
    "\n",
    "pickle.dump(tfidfVectorizer, open(\"../Dataset/tfidf_vectorizer-1.pickle\", \"wb\"))\n",
    "pickle.dump(tokenizer, open(\"../Dataset/tokenizer-1.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(trainTfidfVector, open(\"../Dataset/tfidf_train-1.pickle\", \"wb\"))\n",
    "pickle.dump(trainSequence, open(\"../Dataset/sequence_train-1.pickle\", \"wb\"))\n",
    "pickle.dump(df_1['label'], open(\"../Dataset/label_train-1.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(testTfidfVector, open(\"../Dataset/tfidf_test-1.pickle\", \"wb\"))\n",
    "pickle.dump(testSequence, open(\"../Dataset/sequence_test-1.pickle\", \"wb\"))\n",
    "pickle.dump(df_2['label'], open(\"../Dataset/label_test-1.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = pickle.load(open(\"../Dataset/tfidf_vectorizer-1.pickle\", \"rb\"))\n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "tokenizer = pickle.load(open(\"../Dataset/tokenizer-1.pickle\", \"rb\"))\n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = pickle.load(open(\"../Dataset/tfidf_train-1.pickle\", \"rb\"))\n",
    "df_train = pd.DataFrame(data = trainTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_train)\n",
    "\n",
    "testTfidfVector = pickle.load(open(\"../Dataset/tfidf_test-1.pickle\", \"rb\"))\n",
    "df_test = pd.DataFrame(data = testTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_test)\n",
    "\n",
    "trainSequence = pickle.load(open(\"../Dataset/sequence_train-1.pickle\", \"rb\"))\n",
    "#print(trainSequence)\n",
    "\n",
    "testSequence = pickle.load(open(\"../Dataset/sequence_test-1.pickle\", \"rb\"))\n",
    "#print(testSequence)\n",
    "\n",
    "trainLabels = pickle.load(open(\"../Dataset/label_train-1.pickle\", \"rb\"))\n",
    "#print(trainLabels)\n",
    "\n",
    "testLabels = pickle.load(open(\"../Dataset/label_test-1.pickle\", \"rb\"))\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
