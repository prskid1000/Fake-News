{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "id                                                          \n",
      "0   house aide even see letter subscribe stump for...      1\n",
      "1   ever get feeling life roundabout rather straig...      0\n",
      "2   truth might get fired tension intelligence pol...      1\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"../Dataset/csv/train.csv\", header=0, index_col=0)\n",
    "df_1 = df_1.drop(['title','author'], axis = 1)\n",
    "df_1.dropna(inplace = True)\n",
    "#print(df_1.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_1['text'] = df_1['text'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_1['text'] = df_1['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(df_1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "id                                                             \n",
      "20800  alto political process silicon valley leapt fr...      0\n",
      "20801  ready strike near source source attack aircraf...      1\n",
      "20802  native vow stay winter file lawsuit police amn...      0\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"../Dataset/csv/test.csv\", header=0, index_col=0)\n",
    "df_2 = df_2.drop(['title','author'], axis = 1)\n",
    "df_2.dropna(inplace = True)\n",
    "#print(df_2.isnull().sum(axis = 0))\n",
    "\n",
    "#Expanding Contraction, Lower Case and Word Splitting\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [contractions.fix(word, slang=False).lower() for word in x.split()])\n",
    "\n",
    "#Removing Punctuations\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [re.sub(r'[^\\w\\s]','', word)  for word in x])\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Removing Special Charecter and Numbers\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if re.search(\"[@_!#$%^&*()<>?/|}{~:0-9]\", word) == None])\n",
    "\n",
    "#Removing Non-English Words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df_2['text'] = df_2['text'].apply(lambda x: [word for word in x if word in english_words])\n",
    "\n",
    "#Concating Words back to Sentence\n",
    "df_2['text'] = df_2['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Transforming Labels\n",
    "df_2['label'] = df_2['label'].apply(lambda x: int(x))\n",
    "\n",
    "print(df_2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(use_idf=True,stop_words='english')\n",
    "tfidfVectorizer.fit(df_1['text'])  \n",
    "tfidfVectorizer.fit(df_2['text']) \n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = tfidfVectorizer.transform(df_1['text']) \n",
    "testTfidfVector = tfidfVectorizer.transform(df_2['text']) \n",
    "\n",
    "pickle.dump(tfidfVectorizer, open(\"../Dataset/tfidf_vectorizer-1.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(trainTfidfVector, open(\"../Dataset/tfidf_train-1.pickle\", \"wb\"))\n",
    "pickle.dump(df_1['label'], open(\"../Dataset/label_train-1.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(testTfidfVector, open(\"../Dataset/tfidf_test-1.pickle\", \"wb\"))\n",
    "pickle.dump(df_2['label'], open(\"../Dataset/label_test-1.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = pickle.load(open(\"../Dataset/tfidf_vectorizer-1.pickle\", \"rb\"))\n",
    "#print(tfidfVectorizer.get_feature_names())\n",
    "\n",
    "trainTfidfVector = pickle.load(open(\"../Dataset/tfidf_train-1.pickle\", \"rb\"))\n",
    "df_train = pd.DataFrame(data = trainTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_train)\n",
    "\n",
    "testTfidfVector = pickle.load(open(\"../Dataset/tfidf_test-1.pickle\", \"rb\"))\n",
    "df_test = pd.DataFrame(data = testTfidfVector.toarray(),columns = tfidfVectorizer.get_feature_names())\n",
    "#print(df_test)\n",
    "\n",
    "trainLabels = pickle.load(open(\"../Dataset/label_train-1.pickle\", \"rb\"))\n",
    "#print(trainLabels)\n",
    "\n",
    "testLabels = pickle.load(open(\"../Dataset/label_test-1.pickle\", \"rb\"))\n",
    "#print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
