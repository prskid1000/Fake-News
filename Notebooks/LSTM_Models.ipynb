{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy as spy\n",
    "from skimage import color\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input,MaxPooling1D,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D\n",
    "\n",
    "from keras_visualizer import visualizer \n",
    "\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title              author  \\\n",
      "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
      "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
      "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
      "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
      "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
      "\n",
      "                                                text  label  \n",
      "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1  Ever get the feeling your life circles the rou...      0  \n",
      "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4  Print \\nAn Iranian woman has been sentenced to...      1  \n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "train = pd.read_csv('./../Dataset/csv/train.csv')\n",
    "print(train.head())\n",
    "\n",
    "# load test\n",
    "test = pd.read_csv('./../Dataset/csv/test.csv')\n",
    "#print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the Nan Values\n",
    "#print(train.isnull().sum())\n",
    "\n",
    "# check na in test\n",
    "#print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace na\n",
    "train['text'] = train['text'].replace(np.nan, train['title'])\n",
    "train['label'] = train['label'].replace(np.nan, 0)\n",
    "#print(train.isnull().sum())\n",
    "\n",
    "# Replace na\n",
    "test['text'] = test['text'].replace(np.nan, test['title'])\n",
    "test['label'] = test['label'].replace(np.nan, 0)\n",
    "#print(test.isnull().sum())\n",
    "\n",
    "#Convert Float to Int\n",
    "test['label'] = test['label'].apply(lambda x: int(x))\n",
    "#print(test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Depndent feature\n",
    "X_train=train.drop('label',axis=1)\n",
    "y_train=train['label']\n",
    "X_test=test\n",
    "y_test=test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vocabulary size\n",
    "vo_size=500\n",
    "messages=X_train.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "\n",
    "messages_test=X_test.copy()\n",
    "messages_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset Preprocessing\n",
    "\n",
    "#Train Dataset\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "#Test Dataset\n",
    "ps_test = PorterStemmer()\n",
    "corpus_test = []\n",
    "for i in range(0, len(messages_test)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages_test)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ',messages_test['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps_test.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot representation\n",
    "onehot_rep = [one_hot(words, vo_size) for words in corpus]\n",
    "onehot_rep_test = [one_hot(words, vo_size) for words in corpus_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequences\n",
    "sent_length = 1000\n",
    "embedded_doc=pad_sequences(onehot_rep, padding='pre', maxlen=sent_length)\n",
    "embedded_doc_test=pad_sequences(onehot_rep_test, padding='pre', maxlen=sent_length)\n",
    "print(embedded_doc)\n",
    "print(embedded_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedded_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-df6f899f7436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Dataset/embedded_doc.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_doc_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Dataset/embedded_doc_test.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedded_doc' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(embedded_doc, open(\"../Dataset/embedded_doc.pickle\", \"wb\"))\n",
    "pickle.dump(embedded_doc_test, open(\"../Dataset/embedded_doc_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 1000\n",
    "embedded_doc = pickle.load(open(\"../Dataset/embedded_doc.pickle\", \"rb\"))\n",
    "embedded_doc_test = pickle.load(open(\"../Dataset/embedded_doc_test.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 1000) (20800,) (5200,)\n"
     ]
    }
   ],
   "source": [
    "# final data for NN\n",
    "X_final=np.array(embedded_doc)\n",
    "y_final=np.array(y_train)\n",
    "X_final_test=np.array(embedded_doc_test)\n",
    "y_final_test=np.array(y_test)\n",
    "print(X_final.shape,y_final.shape,y_final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 100)         50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                4440      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 54,451\n",
      "Trainable params: 54,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model-1\n",
    "embedding_vector_feature = 100\n",
    "model=Sequential()\n",
    "#Embedding Layer\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16640 samples, validate on 4160 samples\n",
      "16640/16640 [==============================] - 14s 861us/sample - loss: 0.5883 - accuracy: 0.7438 - val_loss: 0.4517 - val_accuracy: 0.8190\n"
     ]
    }
   ],
   "source": [
    "# train model 1\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='.\\logs', histogram_freq=1)\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=1, batch_size=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d7339316bc92b3ee\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d7339316bc92b3ee\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         150000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 220,251\n",
      "Trainable params: 220,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model-2\n",
    "embedding_vector_feature = 300\n",
    "model=Sequential()\n",
    "#Embedding Layer\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model 2\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 300)         150000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 220,451\n",
      "Trainable params: 220,351\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model-3\n",
    "embedding_vector_feature = 300\n",
    "model=Sequential()\n",
    "#Embedding Layer\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model 3\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 300)         150000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1000, 25)          37525     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 500, 25)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                15200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 202,976\n",
      "Trainable params: 202,876\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model-4\n",
    "embedding_vector_feature = 300\n",
    "model=Sequential()\n",
    "#Embedding Layer\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(Conv1D(filters=25, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model 4\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 100)         50000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 25)          12525     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 200, 25)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25)                5100      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 67,651\n",
      "Trainable params: 67,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model-5\n",
    "embedding_vector_feature = 100\n",
    "model=Sequential()\n",
    "#Embedding Layer\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(Conv1D(filters=25, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(LSTM(25))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model 5\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=50, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Test\n",
    "y_pred_final = model.predict_classes(X_final_test)\n",
    "\n",
    "print(f1_score(y_final_test, y_pred_final))\n",
    "print(precision_score(y_final_test, y_pred_final))\n",
    "print(recall_score(y_final_test, y_pred_final))\n",
    "print(accuracy_score(y_final_test, y_pred_final))\n",
    "print(confusion_matrix(py_final_test, py_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_weights(\"lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
